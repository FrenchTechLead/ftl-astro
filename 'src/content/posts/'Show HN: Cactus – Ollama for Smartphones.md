Hacker Newsnew | past | comments | ask | show | jobs | submitloginShow HN: Cactus – Ollama for Smartphones32 points by HenryNdubuaku 1 hour ago | hide | past | favorite | 16 commentsHey HN, Henry and Roman here - we've been building a cross-platform framework for deploying LLMs, VLMs, Embedding Models and TTS models locally on smartphones.Ollama enables deploying LLMs models locally on laptops and edge severs, Cactus enables deploying on phones. Deploying directly on phones facilitates building AI apps and agents capable of phone use without breaking privacy, supports real-time inference with no latency, we have seen personalised RAG pipelines for users and more.Apple and Google actively went into local AI models recently with the launch of Apple Foundation Frameworks and Google AI Edge respectively. However, both are platform-specific and only support specific models from the company. To this end, Cactus:- Is available in Flutter, React-Native & Kotlin Multi-platform for cross-platform developers, since most apps are built with these today.- Supports any GGUF model you can find on Huggingface; Qwen, Gemma, Llama, DeepSeek, Phi, Mistral, SmolLM, SmolVLM, InternVLM, Jan Nano etc.- Accommodates from FP32 to as low as 2-bit quantized models, for better efficiency and less device strain.- Have MCP tool-calls to make them performant, truly helpful (set reminder, gallery search, reply messages) and more.- Fallback to big cloud models for complex, constrained or large-context tasks, ensuring robustness and high availability.It's completely open source. Would love to have more people try it out and tell us how to make it great!Repo: https://github.com/cactus-compute/cactus deepdarkforest 2 minutes ago | next \[–\] > However, both are platform-specific and only support specific models from the companyThis is not true, as you are for sure aware. Google AI edge supports a lot models, including any Litert model from huggingface, pytorch ones etc. \[0\]. Additionally, it's not even platform specific, works for iOS \[1\]. Why lie? I understand that your framework does more stuff like MCP, but I'm sure that's coming for Google's as well. I guess if the UX is really better it can work, but i would also say Ollama's use cases are quite different because on desktop there's a big community of hobbyists that cook up their own little pipelines/just chat to LLMs with local models (apart from the desktop app devs). But on phones, imo that segment is much smaller. App devs are more likely to use the 1st party frameworks, rather than 3rd party. I wouldnt even be surprised if apple locks down at some points some API's for safety/security reasons.\[0\] https://ai.google.dev/edge/mediapipe/solutions/genai/llm\_inf...\[1\] https://ai.google.dev/edge/mediapipe/solutions/genai/llm\_inf...replymatthewolfe 7 minutes ago | prev | next \[–\] For argument's sake, suppose we live in a world where many high-quality models can be run on-device. Is there any concern from companies/model developers about exposing their proprietary weights to the end user? It's generally not difficult to intercept traffic (weights) sent to and app, or just reverse the app itself.replykhalel 9 minutes ago | prev | next \[–\] What do you think about security? I mean, a model with full (or partial) access to the smartphone and internet. Even if it runs locally, isn't there still a risk that these models could gain full access to the internet and the device?replypolitelemon 31 minutes ago | prev | next \[–\] Very nice, good work. I think you should add the chat app links on the readme, so that visitors get a good idea of what the framework is capable of.The performance is quite good, even on CPU.However I'm now trying it on a pixel, and it's not using GPU if I enable it.I do like this idea as I've been running models in termux until now.Is the plan to make this app something similar to lmstudio for phones?replyrshemet 18 minutes ago | parent | next \[–\] appreciate the feedback! Made the demo links more prominent on the README.Some Android models won't support GPU hardware; we'll be addressing that as we move to our own kernels.The app itself is just a demonstration of Cactus performance. The underlying framework gives you the tools to build any local mobile AI experience you'd like.replyfelarof 33 minutes ago | prev | next \[–\] This is cool!We are working on agentic browser (also launched today https://news.ycombinator.com/item?id=44523409 :))Right now we have a desktop version with ollama support, but we want to build a mobile chromium fork with local LLM support. Will check out cactus!replyrshemet 17 minutes ago | parent | next \[–\] great stuff. (good timing for a post given all the comet news too :) )DM me on BF - let's talk!replyttouch 47 minutes ago | prev | next \[–\] very good project!can you tell us more about the use cases that you have in mind? I saw that you're able to run 1-4B models (which is impressive!)replyrshemet 36 minutes ago | parent | next \[–\] Thank you! it goes without saying that the field is rapidly developing, so the use cases range from private AI assistant/companion apps to internet connectivity-independent copilots to powering private wearables, etc.We're currently working with a few projects in the space.For a demo of a familiar chat interface, download https://apps.apple.com/gb/app/cactus-chat/id6744444212 or https://play.google.com/store/apps/details?id=com.rshemetsub...For other applications, join the discord and stay tuned! :)replyjonathan\_11 7 minutes ago | prev | next \[–\] I trust Gary McKinnon credit repair and I stand by them with everything I have. I was not disappointed in trying them and I believe that they work each case/customer with the professionalism and knowledge of getting the job done just like they did on my credit report. I'll recommend them to anyone who needs fast and great results with all seriousness, Thank you… garymckinnonhacks0@gmail.com +1 207-437-3790 That’s their email and mobile number incase you want to try them out as well…replyyrcyrc 17 minutes ago | prev | next \[–\] how do i add RAG / personal assistant features on iOS?replyrshemet 13 minutes ago | parent | next \[–\] you can plug in a vector DB and run Cactus embeddings for retrieval. Assuming you're using React Native, here's an example:https://github.com/cactus-compute/cactus/tree/main/react#emb...(Flutter works the same way)What are you building?replyxnx 42 minutes ago | prev | next \[–\] Is there an .apk for Android?replyrshemet 39 minutes ago | parent | next \[–\] Cactus is a framework - not the app itself. If you're looking for an Android demo, you can go tohttps://play.google.com/store/apps/details?id=com.rshemetsub...Otherwise, it's easy to build any of the example apps from the repo:cd react/example && yarn && npx expo run:androidorcd flutter/example && flutter pub get && flutter runreplymax-privatevoid 48 minutes ago | prev \[–\] They literally vendored llama.cpp and they STILL called it "Ollama for \*". Georgi cannot be vindicated hard enough.replyrshemet 43 minutes ago | parent \[–\] didn't Ollama vendor Llama cpp too?Most projects typically start with llama.cpp and then move away to proprietary kernelsreply Consider applying for YC's Fall 2025 batch! Applications are open till Aug 4 Guidelines | FAQ | Lists | API | Security | Legal | Apply to YC | Contact Search: